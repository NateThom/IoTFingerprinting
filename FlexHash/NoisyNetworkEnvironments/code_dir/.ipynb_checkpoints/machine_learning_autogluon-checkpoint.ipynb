{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import *\n",
    "import utils\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select one of the following: \n",
      "1. Nilsimsa Per-Packet Devices \n",
      "2. Nilsimsa Per-Packet Categories \n",
      "3. Nilsimsa Identical Devices \n",
      "4. FlexHash Identical Devices \n",
      "5. 100x Noise4\n",
      "Select one of the following: \n",
      "1. Cleaned \n",
      "2. Uncleaned1\n",
      "Select one of the following: \n",
      "1. Plug \n",
      "2. Bulb \n",
      "3. Cam1\n",
      "Select one of the following accumulator sizes: \n",
      "128 \n",
      "256 \n",
      "512 \n",
      "10241024\n",
      "Select one of the following window sizes: \n",
      "4 \n",
      "5 \n",
      "65\n",
      "Select one of the following combination sizes: \n",
      "2 \n",
      "3 \n",
      "4 \n",
      "54\n",
      "Garbage collector: collected 73 objects.\n"
     ]
    }
   ],
   "source": [
    "dataset, name_of_current_data = utils.get_dataset()\n",
    "dataset.reset_index(drop=True, inplace=True)\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected %d objects.\" % (collected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Total samples in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 152009 ***\n",
      "*** Samples for device: plug-1 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 16415 (0.10798702708392266%) ***\n",
      "*** Samples for device: plug-2 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 15192 (0.09994145083514792%) ***\n",
      "*** Samples for device: plug-3 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 17811 (0.11717069384049629%) ***\n",
      "*** Samples for device: plug-4 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 21993 (0.14468222276312587%) ***\n",
      "*** Samples for device: plug-5 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 19964 (0.1313343288884211%) ***\n",
      "*** Samples for device: plug-6 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 14779 (0.09722450644369741%) ***\n",
      "*** Samples for device: plug-7 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 30078 (0.19786986296864%) ***\n",
      "*** Samples for device: plug-8 in FlexHash-plug-accum_1024-win_5-combo_4-cleaned: 15777 (0.10378990717654876%) ***\n",
      "Garbage collector: collected 0 objects.\n",
      "*** Dataset Loaded ***\n"
     ]
    }
   ],
   "source": [
    "print(f\"*** Total samples in {name_of_current_data}: {len(dataset.index)} ***\")\n",
    "for device_name in sorted(dataset[\"class\"].unique()):\n",
    "    num_samples = len((dataset[dataset[\"class\"] == device_name]).index)\n",
    "    print(f\"*** Samples for device: {device_name} in {name_of_current_data}: {num_samples} ({num_samples/dataset.shape[0]}%) ***\")\n",
    "\n",
    "# x is the entire dataframe except for the class column\n",
    "x = dataset.drop(['class'], axis=1)\n",
    "\n",
    "# y_original is an unaltered list of all values in the class column\n",
    "y_original = dataset['class'].values.tolist()\n",
    "\n",
    "# y is a dataframe of only the class column and the values have been converted to numeric representation\n",
    "y = dataset['class']\n",
    "counter = 0\n",
    "y_temp = dataset['class'].tolist()\n",
    "for unique_value in sorted(y.unique()):\n",
    "    for index, value in enumerate(y):\n",
    "        if value == unique_value:\n",
    "            y_temp[index] = counter\n",
    "    counter += 1\n",
    "dataset[\"class\"] = y_temp\n",
    "y = dataset['class']\n",
    "labels_numeric = dataset['class'].unique()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, test_size=.2, stratify=y.values)\n",
    "\n",
    "\n",
    "names = list(range(x_train.shape[1]))\n",
    "train_dataset_df = pd.DataFrame(x_train, columns=names)\n",
    "train_dataset_df.insert(train_dataset_df.shape[1], \"class\", y_train)\n",
    "\n",
    "names = list(range(x_test.shape[1]))\n",
    "test_dataset_df = pd.DataFrame(x_test, columns=names)\n",
    "test_dataset_df.insert(test_dataset_df.shape[1], \"class\", y_test)\n",
    "\n",
    "del x, y, y_original, y_temp, labels_numeric, x_train, y_train, x_test, y_test, dataset, names\n",
    "collected = gc.collect()\n",
    "print(\"Garbage collector: collected %d objects.\" % (collected))\n",
    "\n",
    "print(\"*** Dataset Loaded ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of class variable: \n",
      " count    121607.000000\n",
      "mean          3.693529\n",
      "std           2.229264\n",
      "min           0.000000\n",
      "25%           2.000000\n",
      "50%           4.000000\n",
      "75%           6.000000\n",
      "max           7.000000\n",
      "Name: class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model_save_path=f\"agModels-{name_of_current_data}\"\n",
    "\n",
    "train_dataset_td = TabularDataset(train_dataset_df)\n",
    "label = \"class\"\n",
    "print(\"Summary of class variable: \\n\", train_dataset_td[label].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"agModels-FlexHash-plug-accum_1024-win_5-combo_4-cleaned\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (121607 samples, 544.47 MB).\n",
      "\tConsider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.\n",
      "Beginning AutoGluon training ...\n",
      "AutoGluon will save models to \"agModels-FlexHash-plug-accum_1024-win_5-combo_4-cleaned/\"\n",
      "AutoGluon Version:  0.7.0\n",
      "Python Version:     3.10.9\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #33~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Jan 30 17:03:34 UTC 2\n",
      "Train Data Rows:    121607\n",
      "Train Data Columns: 128\n",
      "Label Column: class\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).\n",
      "\t8 unique label values:  [6, 2, 7, 4, 5, 3, 1, 0]\n",
      "\tIf 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Train Data Class Count: 8\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18056.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 543.49 MB (3.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('object', []) : 128 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 128 | ['0', '1', '2', '3', '4', ...]\n",
      "\t2.7s = Fit runtime\n",
      "\t128 features in original data used to generate 128 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 22.33 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 3.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Excluded Model Types: ['NN_TORCH', 'FASTAI']\n",
      "\tFound 'NN_TORCH' model in hyperparameters, but 'NN_TORCH' is present in `excluded_model_types` and will be removed.\n",
      "\tFound 'FASTAI' model in hyperparameters, but 'FASTAI' is present in `excluded_model_types` and will be removed.\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ...\n",
      "\tNo valid features to train KNeighborsUnif_BAG_L1... Skipping this model.\n",
      "Fitting model: KNeighborsDist_BAG_L1 ...\n",
      "\tNo valid features to train KNeighborsDist_BAG_L1... Skipping this model.\n",
      "Fitting model: LightGBMXT_BAG_L1 ...\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n"
     ]
    }
   ],
   "source": [
    "excluded_model_types = ['NN_TORCH', \"FASTAI\"]\n",
    "predictor = TabularPredictor(label=\"class\", path=model_save_path).fit(train_dataset_td, presets=\"best_quality\", num_gpus=1, excluded_model_types=excluded_model_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                      model  score_val  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L3   0.615878    3906.586901  12426.258812                0.016966          20.355004            3       True         19\n",
      "1           LightGBM_BAG_L2   0.606303    1034.203860   4551.722049              984.593742        4337.725071            2       True         11\n",
      "2      LightGBMLarge_BAG_L2   0.604981    1136.267698   4063.576134             1086.657580        3849.579156            2       True         18\n",
      "3         LightGBMXT_BAG_L2   0.595917    1835.318613   4218.599582             1785.708495        4004.602603            2       True         10\n",
      "4            XGBoost_BAG_L2   0.525972     418.834451   8151.637550              369.224333        7937.640571            2       True         17\n",
      "5     ExtraTreesEntr_BAG_L2   0.335604      63.484051    236.716949               13.873933          22.719970            2       True         16\n",
      "6   RandomForestEntr_BAG_L2   0.334216      63.552885    270.593118               13.942767          56.596139            2       True         13\n",
      "7   RandomForestGini_BAG_L2   0.334092      63.569416    251.750515               13.959298          37.753536            2       True         12\n",
      "8     ExtraTreesGini_BAG_L2   0.333979      63.202251    235.806728               13.592133          21.809749            2       True         15\n",
      "9           CatBoost_BAG_L2   0.331909      51.169994    323.029238                1.559876         109.032259            2       True         14\n",
      "10           XGBoost_BAG_L1   0.129344       3.287505     66.646631                3.287505          66.646631            1       True          7\n",
      "11      WeightedEnsemble_L2   0.129344       3.302199     86.366539                0.014694          19.719908            2       True          9\n",
      "12        LightGBMXT_BAG_L1   0.121963       0.779001      8.151877                0.779001           8.151877            1       True          1\n",
      "13          LightGBM_BAG_L1   0.119982       0.708384     10.897408                0.708384          10.897408            1       True          2\n",
      "14     LightGBMLarge_BAG_L1   0.109484       0.763205     16.246541                0.763205          16.246541            1       True          8\n",
      "15    ExtraTreesGini_BAG_L1   0.022927      10.655664     23.029975               10.655664          23.029975            1       True          5\n",
      "16  RandomForestGini_BAG_L1   0.022268      11.258553     32.733199               11.258553          32.733199            1       True          3\n",
      "17    ExtraTreesEntr_BAG_L1   0.021970      11.036882     24.616698               11.036882          24.616698            1       True          6\n",
      "18  RandomForestEntr_BAG_L1   0.021962      11.120924     31.674650               11.120924          31.674650            1       True          4\n",
      "Number of models trained: 19\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_XGBoost'}\n",
      "Bagging used: True  (with 8 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', []) : 128 | ['0', '1', '2', '3', '4', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nthom/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/core/utils/plots.py:138: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n"
     ]
    }
   ],
   "source": [
    "results = predictor.fit_summary(show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TabularPredictor.load(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_td = TabularDataset(test_dataset_df)\n",
    "y_test = test_dataset_td[label]\n",
    "test_data_noLabel = test_dataset_td.drop(columns=[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: accuracy on test data: 0.325910648657869\n",
      "Evaluations on test data:\n",
      "{\n",
      "    \"accuracy\": 0.325910648657869,\n",
      "    \"balanced_accuracy\": 0.32590317986445605,\n",
      "    \"mcc\": 0.22960156408034707\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "y_pred = predictor.predict(test_data_noLabel)\n",
    "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard_df = predictor.leaderboard(test_dataset_td, silent=True)\n",
    "leaderboard_df.to_csv(f\"autogluon_leaderboard_{name_of_current_data}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "These features in provided data are not utilized by the predictor and will be ignored: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127]\n",
      "Computing feature importance via permutation shuffling for 0 features using 5000 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"128 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. Missing columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/features/generators/abstract.py:329\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_in:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/pandas/core/indexes/base.py:6173\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6172\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\\n       ...\\n       '118', '119', '120', '121', '122', '123', '124', '125', '126', '127'],\\n      dtype='object', length=128)] are in the [columns]\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_importance_df \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset_td\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m feature_importance_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m feature_importance_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/tabular/predictor/predictor.py:1955\u001b[0m, in \u001b[0;36mTabularPredictor.feature_importance\u001b[0;34m(self, data, model, features, feature_stage, subsample_size, time_limit, num_shuffle_sets, include_confidence_band, confidence_level, silent)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_shuffle_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1953\u001b[0m     num_shuffle_sets \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m-> 1955\u001b[0m fi_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1956\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1957\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1958\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_confidence_band:\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m confidence_level \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m confidence_level \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py:671\u001b[0m, in \u001b[0;36mAbstractTabularLearner.get_feature_importance\u001b[0;34m(self, model, X, y, features, feature_stage, subsample_size, silent, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m         X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39munused_features)\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 671\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_feature_importance_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    672\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform_features(X)\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/core/trainer/abstract_trainer.py:2177\u001b[0m, in \u001b[0;36mAbstractTrainer._get_feature_importance_raw\u001b[0;34m(self, X, y, model, eval_metric, **kwargs)\u001b[0m\n\u001b[1;32m   2175\u001b[0m model: AbstractModel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model(model)\n\u001b[1;32m   2176\u001b[0m predict_func_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m-> 2177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_permutation_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   2179\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/core/utils/utils.py:759\u001b[0m, in \u001b[0;36mcompute_permutation_feature_importance\u001b[0;34m(X, y, predict_func, eval_metric, features, subsample_size, num_shuffle_sets, predict_func_kwargs, transform_func, transform_func_kwargs, time_limit, silent, log_prefix, importance_as_list, random_state, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subsample \u001b[38;5;129;01mor\u001b[39;00m shuffle_repeat \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    758\u001b[0m     time_start_score \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 759\u001b[0m     X_transformed \u001b[38;5;241m=\u001b[39m X \u001b[38;5;28;01mif\u001b[39;00m transform_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtransform_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m predict_func(X_transformed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_func_kwargs)\n\u001b[1;32m    761\u001b[0m     score_baseline \u001b[38;5;241m=\u001b[39m eval_metric(y, y_pred, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/tabular/learner/abstract_learner.py:262\u001b[0m, in \u001b[0;36mAbstractTabularLearner.transform_features\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_generators:\n\u001b[0;32m--> 262\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/anaconda3/envs/autogluon/lib/python3.9/site-packages/autogluon/features/generators/abstract.py:335\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.transform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    334\u001b[0m             missing_cols\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[0;32m--> 335\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    336\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator:\n\u001b[1;32m    338\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"128 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. Missing columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127']\""
     ]
    }
   ],
   "source": [
    "feature_importance_df = predictor.feature_importance(test_dataset_td)\n",
    "feature_importance_df[\"p_value\"].mean()\n",
    "feature_importance_df[\"p_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
